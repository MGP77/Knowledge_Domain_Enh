#!/usr/bin/env python3
"""
RAG —Å–µ—Ä–≤–∏—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ langchain-gigachat

Copyright (c) 2025. All rights reserved.
Author: M.P.
"""

import os
import logging
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings
import uuid
import re

from config import config

logger = logging.getLogger(__name__)

class SimpleTextSplitter:
    """–ü—Ä–æ—Å—Ç–æ–π text splitter –±–µ–∑ langchain"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        if not text:
            return []
        
        chunks = []
        current_chunk = ""
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'[.!?]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence + ". "
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                if len(sentence) <= self.chunk_size:
                    current_chunk = sentence + ". "
                else:
                    # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
                    words = sentence.split()
                    temp_chunk = ""
                    for word in words:
                        if len(temp_chunk) + len(word) <= self.chunk_size:
                            temp_chunk += word + " "
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk.strip())
                            temp_chunk = word + " "
                    current_chunk = temp_chunk
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class SimpleEmbeddingsProvider:
    """–ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
    
    def __init__(self):
        import hashlib
        import struct
        self.is_available = True
        self.embedding_dim = 384  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä
    
    def _text_to_embedding(self, text: str) -> List[float]:
        """–ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥"""
        import hashlib
        import struct
        
        # –°–æ–∑–¥–∞–µ–º hash –æ—Ç —Ç–µ–∫—Å—Ç–∞
        text_hash = hashlib.sha256(text.encode()).hexdigest()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Å–ª–∞
        embedding = []
        for i in range(0, min(len(text_hash), self.embedding_dim * 2), 2):
            hex_pair = text_hash[i:i+2]
            val = int(hex_pair, 16) / 255.0 - 0.5  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [-0.5, 0.5]
            embedding.append(val)
        
        # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        while len(embedding) < self.embedding_dim:
            embedding.append(0.0)
            
        return embedding[:self.embedding_dim]
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        return [self._text_to_embedding(text) for text in texts]
    
    def get_query_embedding(self, query: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        return self._text_to_embedding(query)

class GigaChatEmbeddingsProvider:
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ langchain-gigachat"""
    
    def __init__(self, model_name: str = "EmbeddingsGigaR"):
        self.model_name = model_name
        self.cert_path = config.MTLS_CLIENT_CERT
        self.key_path = config.MTLS_CLIENT_KEY
        self.verify_ssl = config.MTLS_VERIFY_SSL
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
        self.is_available = self._check_certificates()
        
        if self.is_available:
            try:
                from langchain_gigachat import GigaChatEmbeddings
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–º endpoint
                self.embeddings = GigaChatEmbeddings(
                    model=model_name,
                    base_url=config.GIGACHAT_BASE_URL,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π endpoint
                    ca_bundle_file=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ CA
                    cert_file=self.cert_path,
                    key_file=self.key_path,
                    verify_ssl_certs=self.verify_ssl
                )
                
                logger.info(f"‚úÖ GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã (–º–æ–¥–µ–ª—å: {model_name}, endpoint: {config.GIGACHAT_BASE_URL})")
                
            except ImportError as e:
                logger.warning(f"langchain-gigachat –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
                self.is_available = False
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                self.is_available = False
    
    def _check_certificates(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤"""
        try:
            if not os.path.exists(self.cert_path) or not os.path.exists(self.key_path):
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
            with open(self.cert_path, 'r') as f:
                cert_content = f.read()
                if "# Certificate Placeholder" in cert_content:
                    return False
            
            with open(self.key_path, 'r') as f:
                key_content = f.read()
                if "# Private Key Placeholder" in key_content:
                    return False
            
            return True
            
        except Exception:
            return False
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        if not self.is_available:
            raise ValueError("GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º embed_documents –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            embeddings = self.embeddings.embed_documents(texts)
            return embeddings
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ GigaChat: {e}")
            return []
    
    def get_query_embedding(self, query: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        if not self.is_available:
            raise ValueError("GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã")
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º embed_query –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞)
            embedding = self.embeddings.embed_query(query)
            return embedding
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞ GigaChat: {e}")
            return []

class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self):
        self.client = None
        self.collection = None
        self.text_splitter = None
        self.embedding_provider = None
        self.is_available = False
        self._initialize()
    
    def _initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Chroma –∏ embedding –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è Chroma –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(config.CHROMA_DB_PATH, exist_ok=True)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Chroma
            self.client = chromadb.PersistentClient(
                path=config.CHROMA_DB_PATH,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.collection = self.client.get_or_create_collection(
                name=config.CHROMA_COLLECTION_NAME,
                metadata={"description": "Knowledge base for RAG"}
            )
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è text splitter
            self.text_splitter = SimpleTextSplitter(
                chunk_size=config.CHUNK_SIZE,
                chunk_overlap=config.CHUNK_OVERLAP
            )
            
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            self._initialize_embedding_provider()
            
            if self.embedding_provider:
                self.is_available = True
                logger.info("‚úÖ RAG —Å–µ—Ä–≤–∏—Å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                logger.info(f"üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {self.collection.count()}")
                logger.info(f"üîß –ü—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {type(self.embedding_provider).__name__}")
            else:
                logger.warning("‚ö†Ô∏è –ù–∏ –æ–¥–∏–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG —Å–µ—Ä–≤–∏—Å–∞: {e}")
            self.is_available = False
    
    def _initialize_embedding_provider(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        gigachat_provider = GigaChatEmbeddingsProvider(
            model_name=getattr(config, 'GIGACHAT_EMBEDDING_MODEL', 'EmbeddingsGigaR')
        )
        
        if gigachat_provider.is_available:
            self.embedding_provider = gigachat_provider
            logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
        else:
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            logger.warning("‚ö†Ô∏è GigaChat —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—Ä–æ–≤–∞–π–¥–µ—Ä")
            self.embedding_provider = SimpleEmbeddingsProvider()
            logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (fallback)")
    
    def check_availability(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ RAG —Å–µ—Ä–≤–∏—Å–∞"""
        return self.is_available and self.collection is not None and self.embedding_provider is not None
    
    def add_document(self, content: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ RAG –±–∞–∑—É —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —á–∞–Ω–∫–∞—Ö"""
        if not self.check_availability():
            logger.error("RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return {"success": False, "chunks_added": 0, "error": "RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"}
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.text_splitter.split_text(content)
            
            if not chunks:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏")
                return {"success": False, "chunks_added": 0, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"}
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
            chunk_texts = [chunk for chunk in chunks if chunk.strip()]
            if not chunk_texts:
                logger.warning("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
                return {"success": False, "chunks_added": 0, "error": "–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"}
            
            embeddings = self.embedding_provider.get_embeddings(chunk_texts)
            if not embeddings or len(embeddings) != len(chunk_texts):
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤")
                return {"success": False, "chunks_added": 0, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"}
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
            documents = []
            metadatas = []
            ids = []
            chunk_embeddings = []
            
            for i, (chunk, embedding) in enumerate(zip(chunk_texts, embeddings)):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
                chunk_id = str(uuid.uuid4())
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∞
                chunk_metadata = metadata.copy()
                chunk_metadata.update({
                    "chunk_index": i,
                    "chunk_size": len(chunk),
                    "total_chunks": len(chunk_texts)
                })
                
                documents.append(chunk)
                metadatas.append(chunk_metadata)
                ids.append(chunk_id)
                chunk_embeddings.append(embedding)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
                embeddings=chunk_embeddings
            )
            
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}'")
            return {"success": True, "chunks_added": len(documents)}
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {"success": False, "chunks_added": 0, "error": str(e)}
    
    def search(self, query: str, n_results: Optional[int] = None) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ RAG –±–∞–∑–µ"""
        if not self.check_availability():
            logger.error("RAG —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return []
        
        if n_results is None:
            n_results = config.MAX_SEARCH_RESULTS
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_provider.get_query_embedding(query)
            if not query_embedding:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")
                return []
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=min(n_results, self.collection.count()),
                include=["documents", "metadatas", "distances"]
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            formatted_results = []
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    'text': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'score': 1 - results['distances'][0][i]  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º distance –≤ score
                })
            
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(formatted_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...'")
            return formatted_results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ RAG –±–∞–∑—ã"""
        if not self.check_availability():
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "confluence_pages": 0,
                "uploaded_files": 0,
                "status": "unavailable",
                "embedding_provider": "none"
            }
        
        try:
            count = self.collection.count()
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            all_metadata = self.collection.get(include=["metadatas"])
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            unique_sources = set()
            confluence_pages = set()
            uploaded_files = set()
            
            for metadata in all_metadata['metadatas']:
                source = metadata.get('source', 'unknown')
                
                if source == 'confluence':
                    # –î–ª—è Confluence —É—á–∏—Ç—ã–≤–∞–µ–º page_id
                    page_id = metadata.get('page_id', 'unknown')
                    confluence_pages.add(page_id)
                elif source in ['file', 'file_upload']:  # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞
                    # –î–ª—è —Ñ–∞–π–ª–æ–≤ —É—á–∏—Ç—ã–≤–∞–µ–º filename
                    filename = metadata.get('filename', 'unknown')
                    uploaded_files.add(filename)
                
                unique_sources.add(source)
            
            return {
                "total_chunks": count,
                "unique_documents": len(unique_sources),
                "confluence_pages": len(confluence_pages),
                "uploaded_files": len(uploaded_files),
                "status": "available",
                "embedding_provider": type(self.embedding_provider).__name__,
                "embedding_model": getattr(self.embedding_provider, 'model_name', 'GigaChat')
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {
                "total_documents": 0,
                "total_chunks": 0,
                "confluence_pages": 0,
                "uploaded_files": 0,
                "status": "error",
                "error": str(e),
                "embedding_provider": "error"
            }
    
    def clear_database(self) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ RAG –±–∞–∑—ã"""
        if not self.check_availability():
            return False
        
        try:
            # –£–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.client.delete_collection(config.CHROMA_COLLECTION_NAME)
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            self.collection = self.client.get_or_create_collection(
                name=config.CHROMA_COLLECTION_NAME,
                metadata={"description": "Knowledge base for RAG"}
            )
            
            logger.info("‚úÖ RAG –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–µ–Ω–∞")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False

    def get_statistics(self) -> Dict[str, Any]:
        """–ê–ª–∏–∞—Å –¥–ª—è get_stats –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å API"""
        stats = self.get_stats()
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Ñ–æ—Ä–º–∞—Ç—É, –æ–∂–∏–¥–∞–µ–º–æ–º—É –≤ API
        return {
            "total_documents": stats.get("unique_documents", 0),
            "total_chunks": stats.get("total_chunks", 0),
            "confluence_pages": stats.get("confluence_pages", 0),
            "uploaded_files": stats.get("uploaded_files", 0)
        }
    
    def get_sources_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        stats = self.get_stats()
        return {
            "confluence_pages": stats.get("confluence_pages", 0),
            "uploaded_files": stats.get("uploaded_files", 0)
        }
    
    def analyze_chunks(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"""
        if not self.check_availability():
            return {
                "total_chunks": 0,
                "avg_chunk_size": 0,
                "min_chunk_size": 0,
                "max_chunk_size": 0,
                "unique_sources": 0
            }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            all_data = self.collection.get(include=["documents", "metadatas"])
            
            if not all_data['documents']:
                return {
                    "total_chunks": 0,
                    "avg_chunk_size": 0,
                    "min_chunk_size": 0,
                    "max_chunk_size": 0,
                    "unique_sources": 0
                }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            chunk_sizes = [len(doc) for doc in all_data['documents']]
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            sources = set()
            source_distribution = {}
            
            for metadata in all_data['metadatas']:
                source = metadata.get('source', 'unknown')
                sources.add(source)
                
                if source in source_distribution:
                    source_distribution[source] += 1
                else:
                    source_distribution[source] = 1
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            distribution_text = ""
            for source, count in source_distribution.items():
                distribution_text += f"{source}: {count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤<br>"
            
            return {
                "total_chunks": len(chunk_sizes),
                "avg_chunk_size": round(sum(chunk_sizes) / len(chunk_sizes)) if chunk_sizes else 0,
                "min_chunk_size": min(chunk_sizes) if chunk_sizes else 0,
                "max_chunk_size": max(chunk_sizes) if chunk_sizes else 0,
                "unique_sources": len(sources),
                "distribution": distribution_text.rstrip("<br>")
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {e}")
            return {
                "total_chunks": 0,
                "avg_chunk_size": 0,
                "min_chunk_size": 0,
                "max_chunk_size": 0,
                "unique_sources": 0,
                "error": str(e)
            }
